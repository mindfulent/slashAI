# Database Restore Workflow
# See docs/enhancements/008_DATABASE_BACKUP.md for full specification

name: Database Restore

on:
  workflow_dispatch:
    inputs:
      backup_filename:
        description: 'Backup filename to restore (e.g., slashai_pre-migration_20260112_150000.dump)'
        required: true
        type: string
      target_database:
        description: 'Target database name (will be created if not exists)'
        required: true
        default: 'slashai_restored'
        type: string
      confirm:
        description: 'Type RESTORE to confirm'
        required: true
        type: string

jobs:
  restore:
    runs-on: ubuntu-latest
    if: ${{ inputs.confirm == 'RESTORE' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-16
          pip install boto3

      - name: Download backup from Spaces
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACES_REGION: ${{ secrets.DO_SPACES_REGION }}
          DO_SPACES_BUCKET: ${{ secrets.DO_SPACES_BUCKET }}
        run: |
          python << 'EOF'
          import boto3
          import os
          from botocore.config import Config

          s3 = boto3.client(
              's3',
              endpoint_url=f"https://{os.environ['DO_SPACES_REGION']}.digitaloceanspaces.com",
              aws_access_key_id=os.environ['DO_SPACES_KEY'],
              aws_secret_access_key=os.environ['DO_SPACES_SECRET'],
              config=Config(signature_version='s3v4'),
              region_name=os.environ['DO_SPACES_REGION']
          )

          filename = "${{ inputs.backup_filename }}"
          key = f"db-backups/{filename}"

          print(f"Downloading {key}...")
          s3.download_file(os.environ['DO_SPACES_BUCKET'], key, filename)
          print(f"Downloaded: {filename}")
          EOF

      - name: Restore to database
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          FILENAME="${{ inputs.backup_filename }}"
          TARGET_DB="${{ inputs.target_database }}"

          echo "Restoring $FILENAME to database: $TARGET_DB"
          echo "NOTE: This creates/replaces the target database."
          echo ""

          # Extract connection parts from DATABASE_URL
          # Format: postgresql://user:pass@host:port/dbname?sslmode=require

          # For now, output instructions (full automation would require admin access)
          echo "============================================"
          echo "MANUAL RESTORE STEPS:"
          echo "============================================"
          echo ""
          echo "1. Connect to your DO database cluster"
          echo "2. Create a new database: CREATE DATABASE $TARGET_DB;"
          echo "3. Run: pg_restore -d \$NEW_DATABASE_URL -v $FILENAME"
          echo ""
          echo "Or restore to the existing database (DESTRUCTIVE):"
          echo "   pg_restore -d \$DATABASE_URL --clean --if-exists -v $FILENAME"
          echo ""
          echo "Backup file is available as artifact."

      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup
          path: ${{ inputs.backup_filename }}
          retention-days: 7
