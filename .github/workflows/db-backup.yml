# Database Backup Workflow
# See docs/enhancements/008_DATABASE_BACKUP.md for full specification

name: Database Backup

on:
  schedule:
    # Daily at 6:00 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Backup type'
        required: true
        default: 'manual'
        type: choice
        options:
          - daily
          - pre-migration
          - manual
      notify_discord:
        description: 'Send Discord notification'
        required: false
        default: true
        type: boolean

env:
  SPACES_BUCKET: ${{ secrets.DO_SPACES_BUCKET }}
  SPACES_PREFIX: db-backups
  RETENTION_DAYS: 30

jobs:
  backup:
    runs-on: ubuntu-latest
    outputs:
      filename: ${{ steps.backup.outputs.filename }}
      size_mb: ${{ steps.backup.outputs.size_mb }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          # Install PostgreSQL 16 client
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-16

          # Install boto3 for Spaces upload
          pip install boto3

      - name: Determine backup type
        id: type
        run: |
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "backup_type=daily" >> $GITHUB_OUTPUT
          else
            echo "backup_type=${{ inputs.backup_type }}" >> $GITHUB_OUTPUT
          fi

      - name: Create database backup
        id: backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          BACKUP_TYPE="${{ steps.type.outputs.backup_type }}"
          FILENAME="slashai_${BACKUP_TYPE}_${TIMESTAMP}.dump"

          echo "Creating backup: $FILENAME"

          # Create backup using custom format (compressed, supports parallel restore)
          pg_dump "$DATABASE_URL" \
            --format=custom \
            --verbose \
            --no-owner \
            --no-acl \
            > "$FILENAME"

          # Get file size (Linux stat syntax)
          SIZE_BYTES=$(stat -c%s "$FILENAME")
          SIZE_MB=$(echo "scale=2; $SIZE_BYTES / 1024 / 1024" | bc)

          echo "Backup complete: $FILENAME ($SIZE_MB MB)"

          echo "filename=$FILENAME" >> $GITHUB_OUTPUT
          echo "size_mb=$SIZE_MB" >> $GITHUB_OUTPUT

      - name: Upload to DO Spaces
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACES_REGION: ${{ secrets.DO_SPACES_REGION }}
        run: |
          python << 'EOF'
          import boto3
          import os
          from botocore.config import Config

          s3 = boto3.client(
              's3',
              endpoint_url=f"https://{os.environ['DO_SPACES_REGION']}.digitaloceanspaces.com",
              aws_access_key_id=os.environ['DO_SPACES_KEY'],
              aws_secret_access_key=os.environ['DO_SPACES_SECRET'],
              config=Config(signature_version='s3v4'),
              region_name=os.environ['DO_SPACES_REGION']
          )

          filename = "${{ steps.backup.outputs.filename }}"
          key = f"${{ env.SPACES_PREFIX }}/{filename}"

          print(f"Uploading {filename} to s3://${{ env.SPACES_BUCKET }}/{key}")

          s3.upload_file(
              filename,
              "${{ env.SPACES_BUCKET }}",
              key,
              ExtraArgs={'ACL': 'private'}
          )

          print("Upload complete")
          EOF

      - name: Prune old backups
        env:
          DO_SPACES_KEY: ${{ secrets.DO_SPACES_KEY }}
          DO_SPACES_SECRET: ${{ secrets.DO_SPACES_SECRET }}
          DO_SPACES_REGION: ${{ secrets.DO_SPACES_REGION }}
        run: |
          python << 'EOF'
          import boto3
          import os
          from datetime import datetime, timezone, timedelta
          from botocore.config import Config

          s3 = boto3.client(
              's3',
              endpoint_url=f"https://{os.environ['DO_SPACES_REGION']}.digitaloceanspaces.com",
              aws_access_key_id=os.environ['DO_SPACES_KEY'],
              aws_secret_access_key=os.environ['DO_SPACES_SECRET'],
              config=Config(signature_version='s3v4'),
              region_name=os.environ['DO_SPACES_REGION']
          )

          bucket = "${{ env.SPACES_BUCKET }}"
          prefix = "${{ env.SPACES_PREFIX }}/"
          retention_days = ${{ env.RETENTION_DAYS }}
          cutoff = datetime.now(timezone.utc) - timedelta(days=retention_days)

          deleted = 0
          paginator = s3.get_paginator('list_objects_v2')

          for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
              for obj in page.get('Contents', []):
                  if obj['LastModified'].replace(tzinfo=timezone.utc) < cutoff:
                      # Keep pre-migration backups longer (don't auto-delete)
                      if 'pre-migration' not in obj['Key']:
                          print(f"Deleting old backup: {obj['Key']}")
                          s3.delete_object(Bucket=bucket, Key=obj['Key'])
                          deleted += 1

          print(f"Pruned {deleted} old backups (kept pre-migration backups)")
          EOF

      - name: Notify Discord - Success
        if: success() && (github.event_name == 'schedule' || inputs.notify_discord)
        continue-on-error: true
        env:
          DISCORD_BOT_TOKEN: ${{ secrets.DISCORD_BOT_TOKEN }}
          DISCORD_CHANNEL_ID: ${{ secrets.DISCORD_BACKUP_CHANNEL_ID }}
        run: |
          python << 'EOF'
          import urllib.request
          import json
          import os

          msg = f":white_check_mark: **Database backup complete**\n"
          msg += f"Type: `${{ steps.type.outputs.backup_type }}`\n"
          msg += f"File: `${{ steps.backup.outputs.filename }}`\n"
          msg += f"Size: ${{ steps.backup.outputs.size_mb }} MB"

          req = urllib.request.Request(
              f"https://discord.com/api/v10/channels/{os.environ['DISCORD_CHANNEL_ID']}/messages",
              data=json.dumps({'content': msg}).encode(),
              headers={
                  'Authorization': f"Bot {os.environ['DISCORD_BOT_TOKEN']}",
                  'Content-Type': 'application/json',
                  'User-Agent': 'slashAI-Backup (1.0)'
              }
          )
          urllib.request.urlopen(req)
          EOF

      - name: Notify Discord - Failure
        if: failure() && (github.event_name == 'schedule' || inputs.notify_discord)
        continue-on-error: true
        env:
          DISCORD_BOT_TOKEN: ${{ secrets.DISCORD_BOT_TOKEN }}
          DISCORD_CHANNEL_ID: ${{ secrets.DISCORD_BACKUP_CHANNEL_ID }}
        run: |
          python << 'EOF'
          import urllib.request
          import json
          import os

          run_url = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          msg = f":x: **Database backup failed**\n"
          msg += f"Type: `${{ steps.type.outputs.backup_type }}`\n"
          msg += f"[View logs]({run_url})"

          req = urllib.request.Request(
              f"https://discord.com/api/v10/channels/{os.environ['DISCORD_CHANNEL_ID']}/messages",
              data=json.dumps({'content': msg}).encode(),
              headers={
                  'Authorization': f"Bot {os.environ['DISCORD_BOT_TOKEN']}",
                  'Content-Type': 'application/json',
                  'User-Agent': 'slashAI-Backup (1.0)'
              }
          )
          urllib.request.urlopen(req)
          EOF
